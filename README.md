# 使用GBDT做文本分类

## 1. 数据预处理：

### 1.1 读取数据

获取数据集，对数据集进行分析。由于给定的数据集里面包含很多的文件数据，考虑对其先进行读取操作。读取的每个文件都是一个文本文档，将每个文本文档文件都当做一个str存放到数组中。在存放数组时，先读取整个文本内容，判断是否存在于已有的数组中。若存在，则将其从数组中移除。反之，加入到数组中。对于标签的处理是先设置一个字典，其中key值为标签名，value值为标签id。标签id为0到19的数值。设定一个标签id的数组，在每次存放文件的时候，将标签id也存放到标签数组中。

#### 1.1.1 使用os进行数据读取

使用os.listdir进行文件夹读取，并将该数据添加到最开始的文件目录后面(利用os.path.join进行拼接)。如果得到的数据依旧是一个文件夹(使用os.path.isdir()方法)，则将该文件夹的名字和对应的数字编码存放到字典中。然后继续对文件夹进行遍历，由于这个数据集的特殊性，在文件夹中的文件都是数字命名的，于是判断该命名是否为数字(isdigit())，是，则对其进行文件打开操作。其中对于文件的打开则是采用Latin-1的编码方式，Latin-1是ISO-8859-1的别名。这里对于数据还进行了一个操作是判断是否有重复的数据。如果有重复的数据则将其从数据中移除。

![image-20190416204755396](/Users/sivan/Library/Application Support/typora-user-images/image-20190416204755396.png)

#### 1.1.2 使用sklearn进行数据读取

对于sklearn读取文件有很方便的方法进行读取。直接导入datasets这个包，然后使用datasets中的load_files函数进行读取文件，该方法直接读取所有文件，并且会把读取过程中的子文件夹进行保存和数字编码。得到的结果是一个bunch，里面包含了数据集，标签集还有标签集对应的具体含义和每个文件的名字。用.data方法得到所有的数据集，.target方法获得列表标签数据，已经将其转化为数值型。.target_names获取类别标签的具体含义，即每个子文件夹的名字。

![image-20190416211239447](/Users/sivan/Library/Application Support/typora-user-images/image-20190416211239447.png)

### 1.2 数据处理

对于获取到的数组文件，对其进行数据清洗，词干提取，去除停用词操作。其中数据清洗操作主要是对文本中的大小写进行处理，全部转换为小写，去除里面的数字，标点符号等，然后将多个空格合并为一个空格。清洗之后就进行词干提取，采用nltk中的PorterStemmer。之后对数据进行分词和去除停用词操作。其中停用词直接采用nltk中的停用词表。

### 1.3 划分数据

采用sklearn中的train_test_split方法进行随机划分训练集和测试集。其中测试集所占的比重是0.3。

## 2. 数据向量化：

经过数据处理之后的数据需要将其进行向量化处理。采用的是sklearn中的CountVectorizer和TfidfTransformer的结合。

### 2.1 CountVectorizer

获取数据的词频信息，提取文本特征。对每个训练文本它只考虑每种词汇在该训练文本中出现的次数。

### 2.2 TfidfTransformer

TF-IDF是一种常用于文本处理的方式。将获取到的词频信息转化为词频频率。

## 3. 训练模型

GBDT是一个迭代的决策树算法。直接调用sklearn里面的GBDT库，设置迭代次数进行训练即可。

也利用朴素贝叶斯模型和SVM模型进行训练，得到的结果进行相应的比较。发现GBDT的效果要优于这两个模型。